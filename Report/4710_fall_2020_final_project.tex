\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Our custom theorems/properties
\newtheorem{prp}{Property}
\newtheorem{proofprp}{}

\newtheorem{subprp}{Property}[proofprp]
\newtheorem{thm}[subprp]{Theorem}
\newtheorem{defn}[subprp]{Definition}

% -----------------------------------------------------------------
%                   end header
%------------------------------------------------------------------

\begin{document}
\title{An Improved K-Means Clustering Algorithm for One Dimensional Data}

\author{\IEEEauthorblockN{Ryan Froese}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University of Manitoba} \\
Winnipeg, MB, Canada \\
froeser5@myumanitoba.ca}
\and
\IEEEauthorblockN{James Klassen}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University of Manitoba} \\
Winnipeg, MB, Canada \\
klass167@myumanitoba.ca}
\and
\IEEEauthorblockN{Tyler Loewen}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University of Manitoba} \\
Winnipeg, MB, Canada \\
loewent4@myumanitoba.ca}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
data mining, k-means, clustering, algorithm
\end{IEEEkeywords}

\section{Introduction}
Clustering is a data mining technique used to arrange large amounts of data into distinct clusters. It is used across many fields such as image processing, pattern recognition, bioinformatics, spatial data analysis, and more. The idea of a clustering algorithm is to create clusters containing data points where every data point in the same cluster is more similar to one another than to the data points in other clusters (i.e., maximizing the intra-class similarities within a cluster and minimizing inter-class similarities between other clusters). Thus, creating a set of data points in a cluster that are locally more similar to each other than to data points in other clusters. In **Section 2** we start by describing the original k-means clustering algorithm in detail along with some of its shortcomings. We then discuss some commonly used approaches to improving the k-means clustering algorithm along with a short description of our proposed improvement. In **Section 3** we fully describe our proposed improved k-means clustering algorithm. We include pseudo code for our algorithm and a formal proof of the time complexity of a single iteration. In **Section 4** we analyze the experimental results of our algorithm and compare between both artificial data sets with different sizes and distributions and a real-world data set. In **Section 5** we use a real-world data set containing oceanic and atmospheric data set combined with our clustering algorithm to visualize geospatial data. Lastly, **Section 6** provides a summary of our findings and potential improvements of our algorithm in the future.

\section{Background}

One of the major approaches to clustering is the partitioning method. The partitioning method take a dataset of size *n* and partitions it into some *k* number of partitions (clusters) where *k<=n*, each cluster contains at least one data point, and each data point belongs to exactly one cluster (i.e., distinct clusters). It then evaluates the similarities between items in each cluster using a distance function through an iterative process until all data points are in their final cluster. The centre of a cluster can be calculated using the centroid or medoid methods. The centroid of a cluster is determined by calculating the average of every data point in the cluster while the medoid of a cluster is determined by calculating the median data point of the cluster. \par

One of the most commonly used clustering algorithms is the k-means algorithm [cite]. This algorithm is simple to understand and implement and can be used across a large variety of problems [cite]. During the initial phase of the k-means algorithm, k random values are chosen from the dataset and assigned as the initial means of each k cluster where k is the number of clusters [cite]. In the second phase, the distance from each data point in a cluster to the centroid of every other cluster is calculated. If a data point is closer to a different cluster than the one it currently belongs to, the data point is moved to the closer cluster. After one iteration is complete and all data points have been moved, the centroids for each cluster are recomputed. This process continues until none of the data points move to a different cluster which occurs when the centroids of each partition do not change. One of the problems with the k-means algorithm is the random selection of initial cluster centroids. This random selection has two main effects. First, it makes the algorithm non-deterministic in the results it produces. This means the amount of data points in each cluster can vary greatly depending on initial means chosen [cite]. Second, for large datasets, a bad random selection of initial cluster centroids can greatly reduce the performance of the algorithm due to having to take more iterations to move data points between clusters.

There have been many attempts to improve the k-means clustering algorithm....



\section{Body}

\subsection{Algorithm Overview}
\noindent\fbox{
    \parbox{225pt}{
        \textbf{Normal K-mean algorithm:}
        \begin{itemize}
            \item Inputs
                \begin{itemize}
                    \item Dataset: All data points in any order
                    \item K: Number of clusters
                    \item Input means: Current set of cluster means
                \end{itemize}
            \item Outputs
                \begin{itemize}
                    \item Output means: Updated means for each cluster
                \end{itemize}
            \end{itemize}
    }
}
\vspace{15px}

\textbf{Steps:}
\begin{enumerate}
    \item For each data point, determine which cluster it’s closest to by calculating its distance to each cluster’s mean.
    \item Calculate the mean of each cluster using the data from step 1 as Output means
    \item Return Output means
\end{enumerate}


\noindent\fbox{
    \parbox{225pt}{
        % Similarly define the algorithm for determining the original centroids
        \textbf{Algorithm 1: f-cluster algorithm}
        \begin{itemize}
            \item Inputs
                \begin{itemize}
                    \item Dataset: All data points in ascending order
                    \item K: Number of clusters
                    \item Input means: Current set of cluster means in ascending order
                    \item Cluster sums: The sum of data points in each cluster
                    \item Cluster counts: The number of data points in each cluster
                \end{itemize}
            \item Outputs
                \begin{itemize}
                    \item Output means: Updated means for each cluster
                    \item Cluster sums: Updated sums of data points in each cluster
                    \item Cluster counts: Updated numbers of data points in each cluster
                \end{itemize}
        \end{itemize}
    }
}
\vspace{15px}

\textbf{Steps:}
\begin{enumerate}
    \item Determine the indexes that divide each cluster using the Cluster counts array.
    \item Compute the average of the cluster means adjacent to the cluster borders (e.g. (Input means[2]+Input means[3])/2)
    \item Repeat
    \item If the data point immediately before the cluster border is greater than or equal to the corresponding adjacent cluster average, move the cluster border to the left until that is no longer the case. Update the sums and counts for each cluster as you do this. Note: We do not update the mean of each cluster until the end.
    \item If the data point immediately after the cluster border is less than the corresponding adjacent cluster average, move the cluster border to the right until that is no longer the case. Update the sums and counts for each cluster as you do this.
    \item If the data point immediately before the cluster border is less than the corresponding adjacent cluster average, AND the data point immediately after the cluster border is greater than or equal to the corresponding adjacent cluster average, do not move the cluster border and skip to the next cluster border.
\end{enumerate}

\textbf{For each cluster border:}
\begin{enumerate}
    \item Calculate the updated mean of each cluster as the updated sum divided by the updated count as Output means.
    \item Delete any clusters that have zero items
    \item Return Output means, Cluster sums, Cluster counts.
\end{enumerate}

\subsection{Proof of Correctness}

This algorithm needs a couple requirements in order to function properly:
\begin{enumerate}
    \item There must be no duplicate cluster centroids
    \item Every cluster must contain at least one item in it
\end{enumerate}

Note: if a data point is the same distance from two cluster centroids, this algorithm favours the cluster with the larger centroid.
This case is ignored in this proof, as the chance of this happening is vanishingly small in real world datasets due to floating point precision.

To prove correctness of the algorithm, we must prove the following properties
\begin{prp}\label{closest-centroid}Each item in the dataset will be assigned to the cluster with the closest centroid, and\end{prp}
\begin{prp}\label{border-cross}The cluster borders cannot cross each other (even if the algorithm is done in parallel), as this would cause sum/count calculations to be incorrect.\end{prp}

\subsubsection{Algorithm inputs}
\begin{itemize}
\item $D = \{d_{11}, d_{12}, \dots , d_{1n_1}, \dots, d_{kn_k}\}$ where the dataset $D$ contains elements $d_{ij}$ where $i$ is the cluster and $j$ is the index within the cluster. 
\item $k$, the number of cluster where $k \geq 2$
\item $C = \{c_1, \dots, c_k\}$ where $C$ is a set containing the centroids of each cluster where each $c$ is unique and $c_1 < c_2 < \dots < c_k$. This sequence is constant until the very end of the algorithm.
\item $S = \{s_1, \dots, s_k\}$ where $S$ is a set containing the sums of data points in each cluster
\item $N = \{n_1, \dots, n_k\}$ where $N$ is a set containing the number of data points in each cluster
\item $N_{\text{sum}} = {n_1 + \dots + n_k}$ where $N_{\text{sum}}$ is the total number of data points in $D$
\end{itemize}

\subsubsection{}
In this proof we use the concept of cluster borders separating the data set into its clusters.
This is an integral concept as the algorithm leans heavily on it.
In order to better visualize the intuition of the algorithm, we model the data and associated clusters as a sorted list of data points in clusters separated by cluster borders, i.e. the first border separates clusters 1 and 2, etc.. The set of all cluster borders positions is $B = b_1, \dots, b_{k-1}$. Visually, this looks like the following diagram:

\begin{equation}\begin{aligned}
D=
\underbrace{d_{11}, d_{12}, \dots, d_{1x_1}}_{\substack{s_1 = d_{11} + \dots + d_{1x_1} \\  c_1 = \frac{s_1}{x_1}}}
\overbrace{|}^{\substack{b_1 = \\ x_1}}
% \\
\underbrace{d_{21}, d_{22}, \dots, d_{2x_2}}_{\substack{s_2 = d_{21} + \dots + d_{2x_2} \\  c_2 = \frac{s_2}{x_2}}}
\overbrace{|}^{\substack{b_2 = \\ x_1 + x_2}}
\dots \\
\overbrace{|}^{\substack{b_{k-1} = \\ x_1 + x_2 + \dots + x_{k-1}}}
\underbrace{d_{k1}, d_{k2}, \dots, d_{kx_k}}_{\substack{s_k = d_{k1} + \dots + d_{kx_k} \\  c_k = \frac{s_k}{x_k}}}
\end{aligned}
\end{equation}

\noindent\fbox{
    \parbox{225pt}{
        \subsubsection{Proof outline for Property \ref{closest-centroid}}
        \textbf{Proof outline for Property \ref{closest-centroid}}
        \begin{proofprp}\end{proofprp}

        % \textbf{Property 1.1:}
        \begin{subprp}\label{1-1}
            Data points can only be closest to one of the clusters they’re adjacent to or inside of, i.e. $d_{ij}$ is closest to either $c_{i-1}, c_i$ or $c_{i+1}$
        \end{subprp}

        % \textbf{Definition 1.2:}
        \begin{defn}\label{1-2}
            Definition: a cluster border $b_i$ is in the correct location if all data points before $b_i$ are closer to $c_i$ than $c_{i+1}$, and all data points after $b_i$ are closer to $c_{i+1}$ than $c_i$.
        \end{defn}

        % \textbf{Property 1.3:}
        \begin{subprp}\label{1-3}
            An equivalent statement: If a cluster border $b_i$ is in the correct location, the item $x$ immediately before $b_i$ satisfies $x < (c_i+c_{i+1})/2$, and the item $y$ immediately after $b_i$ satisfies $y > (c_i+c_{i+1})/2$
        \end{subprp}
        
        % \textbf{Property 1.4:}
        \begin{subprp}\label{1-4}
            Once the first $i$ cluster borders have been put in the correct location, it follows that all data points before $b_i$ have been assigned to the correct cluster
        \end{subprp}
        
        % \textbf{Property 1.5:}
        \begin{subprp}\label{1-5}
            Once the last cluster border $b_{k-1}$ has been put in the correct location, all data points after $b_{k-1}$ have been correctly assigned to the last cluster.
        \end{subprp}

        \textbf{Conclusion:}
        By properties 1.4 and 1.5, once the algorithm terminates, all data points have been assigned to the correct cluster.
    }
}
\vspace{15px}

\textbf{Proof of Property \ref{closest-centroid}}

Recall that:
\begin{enumerate}
\item $c_1 < c_2 < \dots < c_k$
\item $d_{11} \leq d12 \leq \dots \leq d_{1n_1} \leq \dots \leq d_{k1} \leq \dots \leq d_{kn_k}$
Due to the properties of means, we know that for any cluster $i$:
$d_{i1} \leq c_i \leq din_i$
\end{enumerate}

\textbf{Property \ref{1-1}}
We are now going to prove that data points can only be closest to one of the clusters they’re adjacent to or inside of,
i.e. $d_{ij}$ is closest to either $c_{i-1}$, $c_i$, or $c_{i+1}$.

Given an item $d_{ij}$ in a cluster $i$ where $j$ is the index of the item inside the cluster, which cluster centroids could $d_{ij}$ conceivably be closest to?
If $i=1$, then it should make sense that $d_{ij}$ is either closest to $c_1$ or $c_2$. It wouldn’t make sense for it to be closer to $c_3$ (or any centroid higher than that), since $c_3 > c_2$.
Putting it more formally: We want to minimize the L2 Norm in 1 dimension $|d_{1i} - c_a|$ over the variable $a$.
By the algebraic definition of absolute values, $|d_{1i}-c3| = c_3-d_{1i}$ because $c_3 > d_{1i}$ and $|d_{1i}-c_2| = c_2-d_{1i}$ because $c_2 \geq d_{1i}$.
Then it follows that:
$|d_{1i} - c_3| > |d_{1i} - c_2| \iff c_3 - d_{1i} > c_2 - d_{1i} \iff c_3 > d_2$ which is one of our given properties.

This same argument applies to any cluster $i$: Data points can only be closest to one of the clusters they’re adjacent to or inside of
i.e. $d_{ij}$ is closest to either $c_{i-1}, c_i$, or $c_{i+1}$

\textbf{Definition \ref{1-2}}
For a cluster border $b_i$ to be in the correct position, all data points to the left of $b_i$ are closer to $ci$ than $c_{i+1}$, and all data points to the right of $bi$ are closer to $c_{i+1}$ than $ci$.

\textbf{Property \ref{1-3}}
For convenience, define $m_i = (c_i+c_{i+1})/2$ as the mean of two adjacent cluster centroids.

\textbf{Lemma:}
if an arbitrary element $d_{ij}$ is less than $m_i$, then it’s closer to $c_i$ than $c_{i+1}$.
Additionally, if an arbitrary element $d_{ij}$ is greater than $m_i$, then it’s closer to $c_{i+1}$ than $c_i$.

\begin{itemize}
    \item If $d_{ij} \leq c_i$ (and hence automatically closer to $c_i$ than $c_{i+1}$):
    \begin{itemize}
        \item Then $d_{ij} < m_i$ since $m_i > c_i$
    \end{itemize}
    \item If $d_{ij} > c_i:$
    \begin{itemize}
        \item Then $|d_{ij} - c_i| = d_{ij} - c_i$ and $|d_{ij}-c_{i+1}| = c_{i+1}-d_{ij}$ (since $d_{ij} < c_{i+1}$ must be true), after which it follows that: $(d_{ij}-c_i) < (c_{i+1}-d_{ij}) \iff 2d_{ij} < c_i+c_{i+1} \iff d_{ij} < (c_i+c_{i+1})/2 \iff d_{ij} < m_i$.
        \item This also applies to $(d_{ij}-c_i) > (c_{i+1}-d_{ij})$: 
        \item $(d_{ij}-c_i) > (c_{i+1}-d_{ij}) \iff 2d_{ij} > c_i+c_{i+1} \iff d_{ij} > (c_i+c_{i+1})/2 \iff d_{ij} > m_i$
    \end{itemize}
\end{itemize}

This extends without loss of generality to an arbitrary element $d_{(i+1)j}$ in cluster $i+1$.
Then by the lemma, our condition for a cluster border being in the correct position is then that
the data point $x$ immediately before the border satisfies $x < m_i$, and the data point $y$ immediately after the border satisfies $y > m_i$,
ensuring that both elements have the shortest distance to the centroid of the cluster it belongs to.
Then by the transitive property, all elements before $x$ are also closer to $c_i$ than $c_{i+1}$, and all elements after $y$ are also closer to $c_{i+1}$ than $c_i$.

\textbf{Property \ref{1-4}}
We will now prove that once the first $i$ cluster borders have been put in the correct location, it follows that all data points before border $b_i$ have been assigned to the correct cluster

Our algorithm starts with the first cluster border at position $b_1$, moving it left/right until it’s in the right place.
Since every element before $b_1$ is closer to $c_1$ than $c_2$, and each item must be closest to either $c_1$ or $c_2$,
we can conclude each item before $b_1$ has been assigned to the correct cluster, i.e. the first cluster.

We then move onto the second cluster border at position $b_2$, and adjust that to the left/right until it’s in the right place.
The same argument as before applies; since every item before $b_2$ is closer to $c_2$ than $c_1$ or $c_3$,
and since every item before $b_2$ must be closest to either $c_1$, $c_2$ or $c_3$, everything before $b_2$ must be assigned to the correct cluster.
We can continue this process for $i$ cluster borders, resulting in every data point before $b_i$ being assigned to the correct cluster.

\textbf{Property \ref{1-5}}
We will now prove that once the last cluster border $b_{k-1}$ has been moved into the correct position, all data points after $b_{k-1}$ have been assigned to the correct cluster.

% TODO: use links to properties
Following from property 1.1 and definition 1.2, all data points after $b_{k-1}$ must be closer to ck than ck-1, and all data points after $b_{k-1}$ must be closer to either ck or ck-1. Therefore, once the last cluster border $b_{k-1}$ has been moved into the correct position, all data points after $b_{k-1}$ have been assigned to the correct cluster.

\textbf{Conclusion:}
% TODO: use links to properties
Therefore, by properties 1.4 and 1.5, once the algorithm terminates, all data points have been assigned to the correct cluster.


\noindent\fbox{
    \parbox{225pt}{
        \textbf{Proof outline for Property \ref{border-cross}}
        \begin{proofprp}\end{proofprp}

        % \textbf{Property 2.1:}
        \begin{subprp}\label{2-1}
            As a cluster border is being adjusted, it cannot cross an unadjusted border to the left or right.
        \end{subprp}
        
        % \textbf{Property 2.2:}
        \begin{subprp}\label{2-2}
            As a cluster border is being adjusted, it cannot cross an already-adjusted border to the left or right.
        \end{subprp}

        \textbf{Conclusion:}
        By properties 2.1 and 2.2, cluster borders cannot cross each other, even if the f-cluster algorithm is executed in parallel.
    }
}
\vspace{15px}


\textbf{Proof of Property \ref{border-cross}}
\textbf{Property 2.1:}
Let us start with an un-adjusted cluster border $bi$ surrounded by unadjusted cluster borders $b_{i-1}$ and $b_{i+1}$.
If $bi$ is moved to the left to get to its correct position, it cannot cross to the left of $d_{i1}$ (the data point just to the right of $b_{i-1}$), since $d_{i1} \leq c_i < c_{i+1}$.
This means that $d_{i1}$ is closer to $c_i$ than $c_{i+1}$, but if $b_i$ were to cross to the left of $d_{i1}$, that would imply it was closer to $c_{i+1}$.
This also means that $b_i$ can move at most $n_{i-1}$ data points to the left.

If $b_i$ is instead moved to the right to get to its correct position, it cannot cross to the right of $d_{i+1n_{i+1}}$ (the data point just to the left of $b_{i+1}$).
This is because $d_{i+1n_{i+1}} >= c_{i+1} > c_i$, meaning that $d_{i+1n_{i+1}}$ is closer to $c_i+1$ than $c_i$,
however if $b_i$ crossed to the right of $d_{i+1n_{i+1}}$ that would imply it was closer to $c_i$ than $c_{i+1}$.
This also means that $b_i$ can move at most $n_{i+1}-1$ data points to the right.

Combining the fact that $b_i$ can move at most $n_{i-1}$ spots to the left, or move at most $n_{i+1}-1$ spots to the right, the absolute maximum a cluster border can move in one iteration is $\max(n_{i-1}, n_{i+1}-1)$.

\textbf{Property 2.2:}
Let us start with an unadjusted cluster border $b_i$ surrounded by adjusted cluster borders $b_{i-1}$ and $b_{i+1}$.
If $b_i$ is moved to the left to get to its correct position, it cannot cross to the left of $b_{i-1}$,
as that would imply that an element left of $b_{i-1}$ was closer to $c_{i+1}$ than $c_{i-1}$, when we already know that the element is closest to $c_{i-1}$.

If $b_i$ is instead moved to the right to get to its correct position, it cannot cross to the right of $b_{i+1}$,
as that would imply that an element to the right of $b_{i+1}$ was closer to $c_{i-1}$ than $c_{i+1}$, when we already know that the element is closest to $c_{i+1}$.

\textbf{Conclusion:}
By properties 2.1 and 2.2, cluster borders cannot cross each other, even if the f-cluster algorithm is executed in parallel, and the maximum number of spots a cluster border $b_i$ can be moved is $\max(n_{i-1}, n_{i+1}-1)$

\subsection{Empirical Runtime Analysis}
Since each iteration of the f-cluster algorithm produces identical results to the normal k-means algorithm,
it logically follows that it will take the exact same number of iterations to converge to its final result.
The number of iterations is known to scale with the size of the dataset,
 but is still an area of active research and is highly variable depending on the initial conditions
%  TODO:
 (Note: reference a paper that shows a bound on the number of iterations here),
 and so our analysis of the total runtime will simply include iterations as a variable $i$. 
 \par

We also won’t be giving any mathematical proofs of the run times quoted here as the proof would likely be non-trivial enough to be out of the scope of this project.
\par

For each iteration and each cluster border, we must check if the cluster border needs to be moved, and then move it into the correct position if necessary.
If we say that the total amount the cluster borders moved plus $k-1$ (to account for checking if each cluster border needs to be moved) is $x$,
then the total number of operations done for a single iteration is simply $O(x)$.

For convenience, define $N_\text{sum} = n_1 + \dots + n_k$, so that $N_\text{sum}$ is the number of data points in the dataset

Our first observation is that $x < N_\text{sum}$:
First, note that the maximum number of spots a cluster border $b_i$ can move is $\max(n_{i-1}, n_{i+1}-1)$ as was shown in part 2 of the proof of correctness.
Assume the worst case scenario — that every border except the last moves as far to the left as it can go,
and the last border goes to either the left or right. $b_1$ will move $n_1-1$ spots to the left, $b_2$ will move $n_2-1$ spots to the left, etc... and the last border will move $\max(n_{k-1}-1, n_k-1)$ spots, either to the left or right depending on which is larger.
So we have 

\begin{equation}\begin{aligned}
    x = (n_1-1)+(n_2-1) + \dots + (n_{k-2}-1) + 
    \\ \max(n_{k-1}-1, n_{k-1})+(k-1) \\ 
    = n_1 + n_2 + \dots + n_{k-2} + \max(n_{k-1}-1, n_k-1) + 1 < n_1 + n_2 + \dots + n_{k-2} + \max(n_{k-1}, n_k) < n_1 + n_2 + \dots + n_{k-1} + n_k
\end{aligned}
\end{equation}

thus proving the property that $x < N_\text{sum}$.

As we noted in the proof of correctness, there are at most 3 possibilities for which centroid a given data point is closest to.
This implies a different optimized k-means algorithm in which you only check the nearest 2 or 3 centroids, instead of checking all k centroids as naive k-means does.
The fact that $x < N_\text{sum}$ proves that our algorithm is faster than even this optimized version of k-means.

For the whole algorithm including all iterations, the total number of operations done is simply the sum of x values over all iterations,
which we denote $x_\text{sum}$, making the total number of operations for the clustering portion of k-means $O(x_\text{sum})$.
Below is a plot of how $x_\text{sum}$ scales with the size of the dataset, for a few different distributions of datasets.


% TODO insert figure

As we can clearly see, $x_\text{sum}$ scales linearly with the size of the dataset, although the slope changes based on the distribution.
This is simply the big $O$ constant scaling factor, and we can safely ignore it.
We can then conclude that the actual clustering part of K-means, including all iterations,takes a total of $O(n)$ operations.
Of course, the base requirement to use f-cluster is that the dataset is sorted.
However since real data generally doesn’t come pre-sorted, we can consider the full runtime of clustering a dataset with a size of n using f-cluster to be O(n log n).
This is in stark contrast to other optimized K-means algorithms which take $O(n^2)$ time. (Reference paper with $O(n^2)$ optimized k-means here).

As a side note, if one were to zoom in on the start of the graph posted above, one would notice that the slope isn’t entirely constant.
However this is okay because it quickly converges towards a constant slope as the size of the dataset increases. Below is a graph of $x_\text{sum}/N_\text{sum}$:

In addition, as with normal K-means, our algorithm is parallelizable.
As we noted in the proof of correctness, the cluster dividers cannot cross each other and interfere with sum/count calculations.
This holds true even when done in parallel, and as such parallelizing the algorithm is possible.
If you parallelized both the sorting and the clustering steps (it would be pointless to parallelize just one), 
the total runtime to sort and cluster the data would be $O(\log n + n/(k-1)) = O(n/k)$ (Note: Reference a paper here that mentions the complexity of parallelized sorting algorithms).


\end{document}
